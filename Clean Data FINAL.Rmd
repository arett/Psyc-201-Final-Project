---
  title: "Ted Talks - Clean Data"
author: "Stefanie Holden, Debra Lindsay, Andrew Marin, & Alex Rett"
date: "11/26/2018"
output: html_document
---
  
  ```{r setup, include=FALSE}
```
# packages used 
```{r}

#install.packages("ngram")
#install.packages("anytime")
#install.packages("formattable")
#install.packages("kableExtra")
#install.packages("tidytext")
#install.packages("yarrr")
library(anytime)
library(formattable) 
library(gender)
library(kableExtra) 
library(knitr) 
library(tidytext)
library(tidyverse) 
library(yarrr)
library(ngram)
install.packages("ngram")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Used 

We have two csv files that were downloaded from a Kaggle dataset that crawled the Ted.com website on September 25, 2017.

1. Includes surface information about the talks including speaker, location of talk, date uploaded, talk title, number of comments, number of views, ratings - among other variables. The unique variable for each talk is its url.
https://www.dropbox.com/s/9fbmnkja8vvaw6m/ted_raw.csv

2. Includes the whole transcript for the talk including points of laughter and applause. The unique variable for each talk is its url.
https://www.dropbox.com/s/3spn0pj3jh5b113/transcripts_raw.csv

There are 83 talks that do not have transcripts and these are dropped from the data

```{r}
ted.data <- 
  read.csv("ted_raw.csv",
           na.strings = "")
ted.transcripts <- 
  read.csv("transcripts_raw.csv",
           na.strings = "")
# bind the two data sets together (matching url) so we can have one clean data set
ted.data.transcripts <-
  merge(x = ted.data,
        y = ted.transcripts,
        by = "url",
        no.dups = TRUE)
```

In order to run text analysis on our variables we need them to be recorded as characters.

```{r}
ted.data.transcripts <-
ted.data.transcripts %>% 
mutate(description = as.character(description),
event = as.character(event),
main_speaker = as.character(main_speaker),
name = as.character(name),
speaker_occupation = as.character(speaker_occupation),
title = as.character(title),
transcript = as.character(transcript),
tags = as.character(tags))
```

To start, we'll cut down on the data we have by excluding musical performances, since this isn't interesting to our question about talks specifically

```{r}
# Removal of musical performances #

# make new column with the number of characters in each transcript (since musical perofrmances have low counts)
ted.data.transcripts <- ted.data.transcripts %>%
  mutate(number.char = nchar(transcript))

# filter our original dataset so that all musical performances are removed (removes 40)
ted.data.transcripts <-  ted.data.transcripts %>%
 filter(!(((str_detect(tags, "live music") 
           | str_detect(tags, "cello") 
           | str_detect(tags, "guitar") 
           | str_detect(tags, "performance") 
           | str_detect(tags, "piano")
           | str_detect(tags, "dance")
          | str_detect(tags, "music")
          | str_detect(tags, "creativity") 
          ) 
          & (number.char < 2000)
          )
        | number.char < 50))


```



Now to cut our years so that we're only looking at the year 2006 to the year 2017 (up to 150 prior to the scraped date, b/c of references indicating that the number of views a video gets in this time is representative of it's preicted views)


The time that the videos were uploaded is recorded in unix timestamp (number of seconds from January 1st, 1970 at UTC). 
As this isn't a very meaningful time period it has been converted to a more readable format - year, months, date, and time that the video was published.

```{r}
ted.data.transcripts <-
ted.data.transcripts %>% 
mutate(published_date_readable = as.POSIXct(published_date, origin = "1970-01-01"))
```


```{r}
# filter so that we're only look at the years 2006 to April 28 2017

#make new columns with easy to read publish and film dates
ted.data.transcripts <- ted.data.transcripts %>%
  mutate(publish.date = anydate(ted.data.transcripts$published_date), 
         film.date = anydate(ted.data.transcripts$film_date))

#filter so that we only have videos with a publish date prior to April 28, 2017

ted.data.transcripts <- ted.data.transcripts %>%
  filter(publish.date < "2017-04-28")

# thus far we've removed 40 videos for musical performances, and an additional 109 for the date cutoff
```


To make our data cleaner, we're going to log our views and comments


```{r}
#new columns with log transformations
ted.data.transcripts <-
 ted.data.transcripts %>%
  mutate(logviews = log10(views),
         logcomm = log10(comments))
```

Visualtion of our new views data after log transformation

```{r}
plot.views = ggplot(data = ted.data.transcripts, mapping = aes(published_date_readable, logviews)) +
geom_point(position = 'jitter', size = .8, alpha = .2) +
scale_x_datetime(breaks = '2 years', date_labels = "%Y") +
scale_y_continuous(minor_breaks = 100000) +
geom_smooth(method = lm) +
theme_minimal() +
ggtitle("Views as a Function of Year Published") +
labs(x = "Year Published", y = "Views")
lm(views~published_date, data = ted.data.transcripts) %>% 
anova(); plot.views
```
 #Gender
  
  A question we are interested in is whether the gender of the speaker impacts engagement. To do this we need to extract gender information from the speaker name. We did this using the 'gender' package.

```{r}
#edit the original data so that Speaker Name is separated into first name and last name
ted.data.transcripts <- ted.data.transcripts %>%
  separate(main_speaker, into = c("first_name","last_name"), convert = T)


# generate a data frame of the predicted gender of each name in our dataset of TED names 
ted.data.gender <- ted.data.transcripts %>% 
  distinct(first_name) %>% 
  rowwise() %>% 
  do(results = gender(.$first_name, method = "ssa")) %>% 
  do(bind_rows(.$results))

#rename name column in gender data frame 
names(ted.data.gender)[names(ted.data.gender) == 'name'] <- 'first_name'

#combine the genders generated from the historical data with our original TED data
ted.data.transcripts <- ted.data.transcripts %>%
  left_join(ted.data.gender, by = "first_name")

# count the number of missing values that weren't assigned a gender
sum(is.na(gender.ted.data.combined$gender))
"Improv" %in% ted.data.gender$first_name
## we have 184 people with missing gender ## 
males <- ted.data.transcripts %>%
  filter(gender == "male")
females <- ted.data.transcripts %>%
  filter(gender == "female")

#graph of gender distribution in dataframe
ted.data.transcripts %>%
  subset(.,!is.na(gender)) %>%
  ggplot(aes(x = gender, fill = gender)) +
  geom_histogram(stat = "count") +
  theme_minimal() +
  ggtitle("Distribution of Female to Male Speakers") +
  labs(x = "Gender", y = "Frequency") +
  guides(fill = FALSE)
```


NOW, to publish our cleaned up CSV 

```{r, eval = FALSE}
write.csv(ted.data.transcripts, file = "Cleaned.ted.data.transcripts.csv")
```

A little extra code: adding columns for laughter 

```{r}
#first of all we need to make sure all the transcripts are in lowercase so we can work on them.
#also calculating the number of senteces, laughters, and applauses.
ted.data.transcripts <-
  ted.data.transcripts %>% 
  mutate(transcript = tolower(transcript),
         laugh.count = str_count(transcript,
                                 pattern = "(laughter)"),
         applause.count = str_count(transcript,
                                    pattern = "(applause)"))
```

