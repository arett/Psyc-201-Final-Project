---
title: "Ted Talks - Prelim Data Summary"
author: "Stefanie Holden, Debra Lindsay, Andrew Marin, & Alex Rett"
date: "11/26/2018"
output: html_document
---

```{r setup, include=FALSE}
# packages used 
library(anytime)
library(circlize) 
library(formattable) 
library(gender)
library(ggrepel) 
library(gridExtra) 
library(Hmisc)
library(igraph)
library(kableExtra) 
library(knitr) 
library(RColorBrewer)
library(readr)
library(reshape2)
library(Rmisc)
library(scales)
library(SnowballC)
library(tidytext)
library(tidyverse) 
library(tm)
library(tweenr)
library(widyr)
library(wordcloud)
library(yarrr)
```

# Data Used

We have two csv files that were downloaded from a Kaggle dataset that crawled the Ted.com website on September 25, 2017.

1. Includes surface information about the talks including speaker, location of talk, date uploaded, talk title, number of comments, number of views, ratings - among other variables. The unique variable for each talk is its url.
https://www.dropbox.com/s/9fbmnkja8vvaw6m/ted_raw.csv

2. Includes the whole transcript for the talk including points of laughter and applause. The unique variable for each talk is its url.
https://www.dropbox.com/s/3spn0pj3jh5b113/transcripts_raw.csv

There are 83 talks that do not have transcripts and these are dropped from the data

```{r}
ted.data <- 
  read.csv("ted_raw.csv",
           na.strings = "")


ted.transcripts <- 
  read.csv("transcripts_raw.csv",
           na.strings = "")

# bind the two data sets together (matching url) so we can have one clean data set
ted.data.transcripts <-
  merge(x = ted.data,
        y = ted.transcripts,
        by = "url",
        no.dups = TRUE)
```

The time that the videos were uploaded is recorded in unix timestamp (number of seconds from January 1st, 1970 at UTC). As this isn't a very meaningful time period it has been converted to a more readable format - year, months, date, and time that the video was published.

```{r}
ted.data.transcripts <-
  ted.data.transcripts %>% 
  mutate(published_date_readable = as.POSIXct(published_date, origin = "1970-01-01"))
```

In order to run text analysis on our variables we need them to be recorded as characters.

```{r}
ted.data.transcripts <-
  ted.data.transcripts %>% 
  mutate(description = as.character(description),
         event = as.character(event),
         main_speaker = as.character(main_speaker),
         name = as.character(name),
         speaker_occupation = as.character(speaker_occupation),
         title = as.character(title),
         transcript = as.character(transcript),
         tags = as.character(tags))
```

# Norming Views
It could be the case that the longer a video is online the more views it has simply by the fact that it has been available for a longer time. We looked to see if this was a case.

```{r}
ggplot(data = ted.data.transcripts, mapping = aes(published_date_readable, log10(views))) +
  geom_point(position = 'jitter', size = .8, alpha = .2) +
  scale_x_datetime(breaks = '2 years', date_labels = "%Y") +
  #scale_y_continuous(minor_breaks = 100000) +
  geom_smooth(method = lm) +
  theme_minimal() +
  ggtitle("Views as a Function of Year Published") +
  labs(x = "Year Published", y = "Views")

lm(views~published_date, data = ted.data.transcripts) %>% 
  anova()
```
Date that the video was published does not account for a significant amount of variance in number of views. F(1, 2465) = 1.494, p = 0.221.

We did think it could be possible that the outliers are masking an effect of time on views. So we ran an anlysis to check if time online impacts views without the outliers.

```{r}
#calculating the mean and sd for views
mean.views <-
  mean(ted.data.transcripts$views)
sd.views <-
  sd(ted.data.transcripts$views)

#find outliers that are three sd's above the mean for views and the remaining dataset that is within this range
outliers.big.views <-
ted.data.transcripts %>% 
  filter(views > (3*sd.views + mean.views)) 

data.less.viewoutliers <-
  ted.data.transcripts %>% 
  filter(views < (3*sd.views + mean.views))

#run the same plot and lm without the outliers for high views
ggplot(data = data.less.viewoutliers, mapping = aes(published_date_readable, views)) +
  geom_point(position = 'jitter', size = .8, alpha = .2) +
  scale_x_datetime(breaks = '2 years', date_labels = "%Y") +
  #scale_y_continuous(minor_breaks = 20000) +
  geom_smooth(method = lm) +
  theme_minimal() +
  ggtitle("Views as a Function of Year Published") +
  labs(x = "Year Published", y = "Views")

lm(views~published_date, data = data.less.viewoutliers) %>% 
  anova()
lm(views~published_date, data = data.less.viewoutliers) %>% 
  summary()

```

When we exclude the outliers date that the video was published does  account for a significant amount of variance in number of views, albeit a small amount (R^2 = 0.003). F(1, 2429) = 8.143, p = 0.004. It is interesting that instead of the effect being that older videos that have had more time to be viewed having increased traffic, it is the more recent videos that have higher view counts. We suspect this may be due to Ted.com having an increase of traffic over time as it becomes more popular, alternatively there could simply be more people online in more recent years than there were when the earlier videos were uploaded. 

The outliers are important to our question, however, so we won't be excluding them from the analysis. Instead we will norm the views variable to get a ratio of views/time online in order to account for the recency effect that appears to boost newer videos.

We extracted the date the date the data was scraped from the meta data of the csv's to find there were created  on *September 25, 2017 at 10:14 PM*. Running this through the Unix Timestamp converter gave us a time of *1506370440*.

```{r}
time.offline <- 1506370440

# Checking to see if there are there any videos that have a larger timecode than this (are more recent than when the data was taken offline) to make sure we were correct when finding our meta data
ted.data.transcripts %>% 
  filter(published_date > 1506370440) %>% 
  select(published_date, published_date_readable) %>% 
  arrange(published_date) %>% 
  head(n = 10)

# Subtract publication date from the time the data set was taken offline to give us how long each video has been online.
ted.data.transcripts <-
  ted.data.transcripts %>% 
  mutate(time_online = (time.offline - published_date))

# now create a normed views based on how long a video has been online 
# views / time online (in seconds)
ted.data.transcripts <-
  ted.data.transcripts %>% 
  mutate(normed_views = (views/time_online))

# time online is now recorded in seconds which isn't a very meaningful value, so we recoded it to be time online in days (86400 in 1 day)
ted.data.transcripts <-
  ted.data.transcripts %>% 
  mutate(time_online = (time_online / 86400))

# look at the normed views in a plot with time published
ggplot(data = ted.data.transcripts, mapping = aes(published_date_readable, normed_views)) +
  geom_point(position = 'jitter', size = .8, alpha = .2) +
  scale_x_datetime(breaks = '2 years', date_labels = "%Y") +
  #scale_y_continuous(minor_breaks = .02) +
  geom_smooth(method = lm) +
  theme_minimal() +
  ggtitle("Views/Day as a Function of Year Published") +
  labs(x = "Year Published", y = "Views (views/day)")

lm(normed_views~published_date, data = ted.data.transcripts) %>% 
  anova()
lm(normed_views~published_date, data = ted.data.transcripts) %>% 
  summary()
ted.data.transcripts <- ted.data.transcripts %>%
  mutate(log.views = log(ted.data.transcripts$normed_views))


ggplot(data = ted.data.transcripts, mapping = aes(published_date_readable, log.views)) +
  geom_point(position = 'jitter', size = .8, alpha = .2) +
  scale_x_datetime(breaks = '2 years', date_labels = "%Y") +
  #scale_y_continuous(minor_breaks = .02) +
  geom_smooth(method = lm) +
  theme_minimal() +
  ggtitle("Views/Day as a Function of Year Published") +
  labs(x = "Year Published", y = "Views (views/day)")
```

After norming the views so we have a variable showing views/day we can see that the date that the video was published (including the high view outliers) does account for a significant amount of variance in number of views (R^2 = 0.154). F(1, 2465) = 447.54, p < 0.005. 

***Views & Comments 

Mean Comments (SD) = 192.57(284.83)

```{r}
#Frequency histogram of number of comments
ted.data.transcripts %>%
  ggplot(aes(x = comments)) +
  geom_histogram(aes(y = ..count..), binwidth = 100, color = "black", fill = "lightsteelblue1") +
  geom_density(aes(y = ..density.. * (2467*100)), color = "black") +
  scale_y_continuous('Frequency of Comments') +
  scale_x_continuous('Number of Comments') +
  theme_minimal() +
  ggtitle("Distribution of Comments")


#Frequency histogram of number of comments with 34 outliers removed

mean.comments = mean(ted.data.transcripts$comments)
sd.comments = sd(ted.data.transcripts$comments)

data.less.comment.outliers <- ted.data.transcripts %>%
  filter(comments < (3*sd.comments + mean.comments))

data.less.comment.outliers %>%
  ggplot(aes(x = comments)) +
  geom_histogram(aes(y = ..count..), binwidth = 100, color = "black", fill = "lightsteelblue1") +
  geom_density(aes(y = ..density.. * (2467*100)), color = "black") +
  scale_y_continuous('Frequency of Comments') +
  scale_x_continuous('Number of Comments') +
  theme_minimal() +
  ggtitle("Distribution of Comments (excluding outliers)")

mean(data.less.comment.outliers$comments, na.rm = T)
sd(data.less.comment.outliers$comments, na.rm = T)

```

Mean Views (SD) = 1,740,295(2,527,086)

```{r}
#Frequency histogram of number of views 
ted.data.transcripts %>%
  ggplot(aes(x = views)) +
  geom_histogram(aes(y = ..count..), binwidth = 100000, color = "black", fill = "lightsteelblue1") +
  geom_density(aes(y = ..density.. * (2467*100000)), color = "black") +
  scale_y_continuous('Frequency of Comments') +
  scale_x_continuous('Number of Comments') +
  theme_minimal() +
  ggtitle("Distribution of Views")

#Frequency histogram of number of views with 36 outliers removed
data.less.viewoutliers %>%
  ggplot(aes(x = views)) +
  geom_histogram(aes(y = ..count..), binwidth = 100000, color = "black", fill = "lightsteelblue1") +
  geom_density(aes(y = ..density.. * (2467*100000)), color = "black") +
  scale_y_continuous('Frequency of Comments') +
  scale_x_continuous('Number of Comments') +
  theme_minimal() +
  ggtitle("Distribution of Views (without outliers")
```

Thinking about our outcome variable we will need to consider the colinearity between views and comments so we have run a preliminary pairwise correlation between them.

```{r}
cor.test(ted.data.transcripts$views, ted.data.transcripts$comments)  

#' Scatter plot between views and comments, with outliers removed 

ted.data.transcripts %>%
  ggplot(aes(x = views, y = comments)) +
  geom_point(position = 'jitter', size = .8, alpha = .2) +
  scale_x_continuous('Views') +
  scale_y_continuous('Comments') +
  geom_smooth(method = lm) +
  theme_minimal() +
  ggtitle("Relationship between Views and Comments")
```


Some basic visuals to see which speakers have gotten the most views overall and which have gotten the least views overall.

As a note, these distibutions change when we take into account the normed views variable (views/day) which removes the effect of recency on view counts.

```{r}
#Look at most and least viewed
ten_talks <- arrange(ted.data.transcripts, normed_views, views)
keeps <- c("title", "main_speaker", "normed_views", "speaker_occupation", "film_date", "views")
ten_talks <- subset(ten_talks, select = c("title", "main_speaker", "views", "normed_views", "speaker_occupation", "film_date"))

last_10_talks <- ten_talks[0:10,]

ten_talks <- arrange(ted.data.transcripts, desc(views))
ten_talks <- subset(ten_talks, select = c("title", "main_speaker", "views", "normed_views", "speaker_occupation", "film_date"))
top_10_talks <- ten_talks[0:10,]


#Make a bar chart to visualize the last and top 10 talks based on views
options(repr.plot.width = 15, repr.plot.height = 5)
tilt_theme <- theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(data = last_10_talks, aes(main_speaker, views, fill = views)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = views), vjust = 1.6, color = "white", size = 3) +
  ggtitle("10 talks with least views") +
  labs(x = "Main Speaker", y = "Views") +
  tilt_theme  +
  guides(fill = FALSE)
ggplot(data = top_10_talks, aes(main_speaker, views, fill = views)) +
  geom_bar(position = "dodge", stat = "identity") +
  geom_text(aes(label = views), vjust = 1.6, color = "white", size = 3) +
  ggtitle("10 talks with most views") +
  labs(x = "Main Speaker", y = "Views") +
  tilt_theme +
  guides(fill = FALSE)

```
Top 10 Talks
```{r}
top_10_talks

```

Bottom 10 Talks
```{r}
last_10_talks

```

Some basic visuals to see which speakers have gotten the most comments overall and which have gotten the least comments overall.

```{r}
#Calculate and visulize last and top 10 talks based on comment
ten_talks <- arrange(ted.data.transcripts, comments)
keeps <- c("title", "main_speaker", "views", "comments")
ten_talks <- subset(ten_talks, select = keeps)

last_10_talks <- ten_talks[0:10,]

ten_talks <- arrange(ted.data.transcripts, desc(comments))
ten_talks <- subset(ten_talks, select = keeps)
top_10_talks <- ten_talks[0:10,]

#Visualize
options(repr.plot.width = 15, repr.plot.height = 5)
tilt_theme <- theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(data = last_10_talks, aes(main_speaker, comments, fill = comments)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = comments), vjust = 1.6, color = "white", size = 3) +
  ggtitle("Talks with least number of comments") +
  tilt_theme +
  guides(fill = FALSE)
ggplot(data = top_10_talks, aes(main_speaker, comments, fill = comments)) +
  geom_bar(position = "dodge", stat = "identity") +
  geom_text(aes(label = comments), vjust = 1.6, color = "white", size = 3) +
  ggtitle("Talks with most number of comments") +
  tilt_theme +
  guides(fill = FALSE)

```

Most Commented on Talk
```{r}
top_10_talks
```

Least Commented on Talk
```{r}
last_10_talks
```

Which talks attract the largest amount of discussion? This is adding a new feature: "Discussion quotient", which is simply the ratio of the number of comments to the number of views. (Note that this is not normed views and we will need to address this before we do our final analysis)

```{r}
#Which talks have the largest discussion quotient?

ted.data.transcripts$dis_quo <- ted.data.transcripts$comments/ted.data.transcripts$views
ten_talks <- arrange(ted.data.transcripts, dis_quo)
keeps <- c("title", "main_speaker", "views", "comments", "dis_quo")
ten_talks <- subset(ten_talks, select = keeps)
last_10_talks <- ten_talks[0:10,]

ten_talks <- arrange(ted.data.transcripts, desc(dis_quo))
ten_talks <- subset(ten_talks, select = keeps)
top_10_talks <- ten_talks[0:10,]

```

Highest Discussion Quotient
```{r}
top_10_talks
```


Lowest Discussion Quotient
```{r}
last_10_talks
```



***

#Gender

A question we are interested in is whether the gender of the speaker impacts engagement. To do this we need to extract gender information from the speaker name. We did this using the 'gender' package.

```{r}
#edit the original data so that Speaker Name is separated into first name and last name
ted.data.transcripts <- ted.data.transcripts %>%
  separate(main_speaker, into = c("first_name","last_name"), convert = T)



# generate a data frame of the predicted gender of each name in our dataset of TED names 
ted.data.gender <- ted.data.transcripts %>% 
  distinct(first_name) %>% 
  rowwise() %>% 
  do(results = gender(.$first_name, method = "ssa")) %>% 
  do(bind_rows(.$results))

#rename name column in gender data frame 
names(ted.data.gender)[names(ted.data.gender) == 'name'] <- 'first_name'

#combine the genders generated from the historical data with our original TED data
gender.ted.data.combined <- ted.data.transcripts %>%
  left_join(ted.data.gender, by = "first_name")

# count the number of missing values that weren't assigned a gender
sum(is.na(gender.ted.data.combined$gender))

"Improv" %in% ted.data.gender$first_name

#graph of gender distribution in dataframe
gender.ted.data.combined %>%
  subset(.,!is.na(gender)) %>%
  ggplot(aes(x = gender, fill = gender)) +
  geom_histogram(stat = "count") +
  theme_minimal() +
  ggtitle("Distribution of Female to Male Speakers") +
  labs(x = "Gender", y = "Frequency") +
  guides(fill = FALSE)
```

Some thiings to note regarding gender
1. There is some missing data from talks that did not have a main speaker listed. These talks will not be used in any alalysis that predicts engagement from gender variables.
2. There are some talks that had mutiple speakers listed, or the speaker had more than two names. In this case, the extraction pulled out the first name available (which will be from the first name listed). For the purposes of analysis using gender it may be better to isolate talks that only have 1 speaker as opposed to groups of speakers.
3. The *gender* script used to predict the gender based on speakers first names was unable to categorise some names were not in the database. A quick look at these names reveals they are primarily non-Anglo names. We are looking at the *genderize* r script which has a larger database of names from more languages.

***
#Occupation

```{r}
#Occupation of TED Speaker vs # of appearances

occupation_df <- as.data.frame(table(ted.data.transcripts$speaker_occupation))
occupation_df <- arrange(occupation_df, desc(Freq))
occupation_df <- head(occupation_df, 10)
colnames(occupation_df) <- c("Occupation", "Appearances")
occupation_df$Occupation <- as.character(occupation_df$Occupation)
occupation_df$Occupation <- factor(occupation_df$Occupation, levels = occupation_df$Occupation)
options(repr.plot.width = 5, repr.plot.height = 4)
ggplot(data = occupation_df, aes(factor(Occupation), Appearances, fill = Occupation)) +
  geom_bar(position = "dodge", stat = "identity") +
  geom_text(aes(label = Appearances), vjust = 1.6, color = "white", size = 3) +
  ggtitle("Number of Appearances at a TED talk by Occupation") +
  xlab("Occupation") +
  tilt_theme +
  guides(fill = FALSE)

```

Do some professions tend to attract a larger number of viewers?
Note that this is not based on normed views

```{r}
#Visualize the relationship b/t the top 10 most popular professions and the views that they got
top_occupation_views <- filter(ted.data.transcripts, ted.data.transcripts$speaker_occupation %in% occupation_df$Occupation)
options(repr.plot.width = 8, repr.plot.height = 4)
ggplot(top_occupation_views, aes(x = speaker_occupation, y = views, fill = speaker_occupation)) + 
  geom_boxplot() +
  geom_jitter(shape = 16, position = position_jitter(0.2)) +
  ggtitle("Views based on Speaker Occupation") +
  tilt_theme +
  guides(fill = FALSE)

```

#Themes

What are the most popular Themes of TED talks?

```{r}
#creating a function to separate the themes
perfect_tag <- function(x){
  x <- unlist(strsplit(x, "'"))
  val = x[2]
  for (i in 3:length(x))
    if (nchar(x[i]) > 2)
      val <- c(val, x[i])
  return(val)
}

ted.data.transcripts$processed_tags <- lapply(ted.data.transcripts$tags, perfect_tag)

#List top ten themes
processed_tags <- ted.data.transcripts$processed_tags
length(processed_tags)
processed_tags <- unlist(processed_tags, recursive = FALSE)
length(processed_tags)
processed_tags <- as.data.frame(table(processed_tags))
processed_tags <- arrange(processed_tags, desc(Freq))
head(processed_tags, 10)

#Most popular themes of Ted Talks visualized
processed_tags$processed_tags <- 
  as.character(processed_tags$processed_tags)
processed_tags$processed_tags <- 
  factor(processed_tags$processed_tags, levels = processed_tags$processed_tags)
ggplot(data = head(processed_tags, 10), aes(processed_tags, Freq, fill = processed_tags)) +
  geom_bar(position = "dodge", stat = "identity") +
  geom_text(aes(label = Freq), vjust = 1.6, color = "white", size = 3) +
  tilt_theme +
  labs(x = "Theme", y = "Frequency")
  ggtitle("Most popular themes of Ted Talks") 

```

***

#Transcripts

We plan on conducting analyses of the transcripts, so we wanted to do some preliminary analysis to look at talk durations and word counts

```{r}
#first of all we need to make sure all the transcripts are in lowercase so we can work on them.
#also calculating the number of senteces, laughters, and applauses.
ted.data.transcripts <-
  ted.data.transcripts %>% 
  mutate(transcript = tolower(transcript),
         sentence.num = str_count(transcript, 
                                  pattern = c(". ","! ")),
         laugh.count = str_count(transcript,
                                 pattern = "(laughter)"),
         applause.count = str_count(transcript,
                                    pattern = "(applause)"))
```

